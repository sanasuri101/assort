# 05-03: W&B Weave Integration & Evals

**Goal**: Integrate Weights & Biases Weave to track prompt versions and run automated evaluations.

**Depends on**: 05-01, 05-02

## Requirements
- **Prompt Versioning**: Every call should log which version of the system prompt was used.
- **Automated Evals**: Define a test suite (golden dataset) of conversations.
- **Prompt Optimization Loop**:
    - Identify calls with `transfer` or `abandoned` outcomes.
    - Generate candidate prompt revisions using an optimizer LLM.
    - Score candidates against the test suite.
    - **Safety Gate**: Revisions must achieve avg_score >= 0.5 to be marked deployable.
- **Segmentation**: Version prompts by `provider:specialty` with fallback (Provider -> Specialty -> Base).
- **Test Case Accumulation**: Successful patterns from 05-02 automatically become new test cases.

## Implementation Details

### 1. Weave Setup (`app/learning/evals.py`)
- Initialize `weave.init("assort-health")`.
- Define `Model` class wrapping the Voice Bot logic (or at least the extraction/decision logic).
- Define `Evaluation` pipeline.

### 2. Dataset
- Create `tests/data/golden_conversations.json`:
    - List of `{input: "...", expected_output: "...", expected_tools: [...]}`.
    - Used for scoring.

### 3. Bot Integration
- Update `VoiceBot` to log traces to Weave.
    - Use `@weave.op` decorator on main handler functions.
    - Log attributes: `call_id`, `prompt_version`.

### 4. Eval Script (`scripts/run_evals.py`)
- CLI script to run the evaluation suite against the current system prompt.
- Outputs scores (Accuracy, Tool Usage Match Rate).

## Verification
- **Manual**: Run `python scripts/run_evals.py`. Verify results appear in W&B dashboard.
- **Auto**: `tests/learning/test_evals.py` mocks Weave API to ensure calls are made.
